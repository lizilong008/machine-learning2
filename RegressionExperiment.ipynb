{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets as ds,model_selection as ms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "class Lmodel:\n",
    "    def params_exp(params):\n",
    "        total=0.0\n",
    "        for i in range(len(params)-1):\n",
    "            total +=(params[i]**2)\n",
    "        return total\n",
    "    def get_sgd_iter_set(x_train,y_train,sgd_size):\n",
    "        total_size=len(y_train)\n",
    "        t_range=list(range(0,total_size))\n",
    "        random.shuffle(t_range)\n",
    "        sgd_list=t_range[0:sgd_size]\n",
    "        x_sgd=[]\n",
    "        y_sgd=[]\n",
    "        for i in sgd_list:\n",
    "            x_sgd.append(x_train[i])\n",
    "            y_sgd.append(y_train[i])\n",
    "        return (x_sgd,y_sgd)\n",
    "    def h_x(x_data,params):\n",
    "        '''\n",
    "        f(x)=wx+b\n",
    "        '''\n",
    "        total=0.0\n",
    "        for i in range(len(x_data)):\n",
    "            total +=x_data[i]*params[i]\n",
    "        out=1/(1+math.exp(-total))\n",
    "        #print(\"out:\"+str(out))\n",
    "        return out\n",
    "\n",
    "    def logistic_loss(x_data,y_data,model_params):\n",
    "        '''\n",
    "        calculate loss func \n",
    "        '''\n",
    "        total_loss=0.0\n",
    "        for i in range(len(y_data)):\n",
    "            predict_y=Lmodel.h_x(x_data[i],model_params)\n",
    "            total_loss+=y_data[i]*math.log(predict_y)+(1-y_data[i])*math.log(1-predict_y)\n",
    "        \n",
    "        total_loss/=-len(x_data)\n",
    "        return total_loss\n",
    "\n",
    "    def logistic_grads(x_data,y_data,model_params):\n",
    "        x_grads=[0.0]*len(x_data[0].data)\n",
    "        for i in range(len(y_data)):\n",
    "            predict_y=Lmodel.h_x(x_data[i].data,model_params)\n",
    "            for j in range(len(x_grads)):\n",
    "                x_grads[j]+=(predict_y-y_data[i])*x_data[i][j]/len(x_data)\n",
    "        return x_grads\n",
    "    def update_norm(x_grads,learning_rate):\n",
    "        '''\n",
    "        calculate gradient func \n",
    "        '''\n",
    "        for i in range(len(x_grads)):\n",
    "            model_params[i]+=(-1)*learning_rate*x_grads[i]\n",
    "\n",
    "\n",
    "    def update_nag(x_data,y_data,learning_rate,model_params,d_params,belta):\n",
    "        '''\n",
    "        update params func \n",
    "        '''\n",
    "        prev_model_params=[0.0]*len(model_params)\n",
    "        for i in range(len(prev_model_params)):\n",
    "            prev_model_params[i]=model_params[i]-learning_rate*belta*d_params[i]\n",
    "        grads=Lmodel.logistic_grads(x_data,y_data,prev_model_params)\n",
    "\n",
    "        for i in range(len(model_params)):\n",
    "            d_params[i]=belta*d_params[i]+grads[i]\n",
    "            model_params[i]+=(-1)*learning_rate*d_params[i]\n",
    "    def update_RMSProp(x_grads,learning_rate,model_params,cache,decay_rate):\n",
    "        '''\n",
    "        update params func \n",
    "        '''\n",
    "        for i in range(len(x_grads)):\n",
    "            model_params[i]+=(-1)*learning_rate*x_grads[i]/(np.sqrt(cache)+0.01)#assume eps ==0.01\n",
    "\n",
    "\n",
    "    def update_AdaDelta(x_grads,theta_cache,model_params,gradient_cache):\n",
    "        '''\n",
    "        update params func \n",
    "        '''\n",
    "        eps=0.001\n",
    "        theta=[0.0]*len(x_grads)\n",
    "        for i in range(len(x_grads)):\n",
    "            model_params[i]+=-1*(np.sqrt(theta_cache+eps)/np.sqrt(gradient_cache+eps))*x_grads[i]\n",
    "\n",
    "    def update_Adam(x_grads,learning_rate,beta1,beta2,m,v):\n",
    "        '''\n",
    "        update params func \n",
    "        '''\n",
    "        eps=1e-6\n",
    "        v = beta2*v + (1-beta2)*Lmodel.params_exp(x_grads)\n",
    "        for i in range(len(x_grads)):\n",
    "            m[i] = beta1*m[i] + (1-beta1)*x_grads[i]\n",
    "            model_params[i]+= - learning_rate*m[i]/(np.sqrt(v) + eps)\n",
    "        return(m,v)\n",
    "\n",
    "    def norm_train(x_train,y_train,x_val,y_val,model_params,learning_rate,iter_num,sgd_size):\n",
    "        '''\n",
    "        the whole train process\n",
    "        '''\n",
    "        train_loss_arr=[]\n",
    "        val_loss_arr=[]\n",
    "        for i in range(iter_num):\n",
    "            train_loss=Lmodel.logistic_loss(x_train,y_train,model_params)\n",
    "            val_loss=Lmodel.logistic_loss(x_val,y_val,model_params)\n",
    "\n",
    "            x_sgd,y_sgd=Lmodel.get_sgd_iter_set(x_train,y_train,sgd_size)\n",
    "            temp_x_grads=Lmodel.logistic_grads(x_sgd,y_sgd,model_params)\n",
    "            \n",
    "            Lmodel.update_norm(temp_x_grads,learning_rate)\n",
    "\n",
    "            train_loss_arr.append(train_loss)\n",
    "            val_loss_arr.append(val_loss)\n",
    "        outcome=(train_loss_arr,val_loss_arr)\n",
    "        return outcome\n",
    "    def nag_train(x_train,y_train,x_val,y_val,model_params,d_params,learning_rate,iter_num,sgd_size,belta):\n",
    "        '''\n",
    "        the whole train process\n",
    "        '''\n",
    "        train_loss_arr=[]\n",
    "        val_loss_arr=[]\n",
    "        for i in range(iter_num):\n",
    "            train_loss=Lmodel.logistic_loss(x_train,y_train,model_params)\n",
    "            val_loss=Lmodel.logistic_loss(x_val,y_val,model_params)\n",
    "\n",
    "            x_sgd,y_sgd=Lmodel.get_sgd_iter_set(x_train,y_train,sgd_size)\n",
    "            temp_x_grads=Lmodel.logistic_grads(x_sgd,y_sgd,model_params)\n",
    "            \n",
    "            Lmodel.update_nag(x_sgd,y_sgd,learning_rate,model_params,d_params,belta)\n",
    "\n",
    "            train_loss_arr.append(train_loss)\n",
    "            val_loss_arr.append(val_loss)\n",
    "        outcome=(train_loss_arr,val_loss_arr)\n",
    "        return outcome\n",
    "    def RMSProp_train(x_train,y_train,x_val,y_val,model_params,learning_rate,iter_num,sgd_size,decay_rate):\n",
    "        '''\n",
    "        the whole train process\n",
    "        '''\n",
    "        train_loss_arr=[]\n",
    "        val_loss_arr=[]\n",
    "        cache =0.0\n",
    "        for i in range(iter_num):\n",
    "            train_loss=Lmodel.logistic_loss(x_train,y_train,model_params)\n",
    "            val_loss=Lmodel.logistic_loss(x_val,y_val,model_params)\n",
    "\n",
    "            x_sgd,y_sgd=Lmodel.get_sgd_iter_set(x_train,y_train,sgd_size)\n",
    "            temp_x_grads=Lmodel.logistic_grads(x_sgd,y_sgd,model_params)\n",
    "            if (i==0):\n",
    "                cache=Lmodel.params_exp(temp_x_grads)**2\n",
    "            else:\n",
    "                cache=decay_rate*cache+(1-decay_rate)*Lmodel.params_exp(temp_x_grads)\n",
    "            Lmodel.update_RMSProp(temp_x_grads,learning_rate,model_params,cache,decay_rate)\n",
    "\n",
    "            train_loss_arr.append(train_loss)\n",
    "            val_loss_arr.append(val_loss)\n",
    "        outcome=(train_loss_arr,val_loss_arr)\n",
    "        return outcome\n",
    "    def AdaDelta_train(x_train,y_train,x_val,y_val,model_params,learning_rate,iter_num,sgd_size,decay_rate):\n",
    "        '''\n",
    "        the whole train process\n",
    "        '''\n",
    "        train_loss_arr=[]\n",
    "        val_loss_arr=[]\n",
    "        theta_cache=0.0\n",
    "        gradient_cache=0.0\n",
    "        for i in range(iter_num):\n",
    "            train_loss=Lmodel.logistic_loss(x_train,y_train,model_params)\n",
    "            val_loss=Lmodel.logistic_loss(x_val,y_val,model_params)\n",
    "\n",
    "            x_sgd,y_sgd=Lmodel.get_sgd_iter_set(x_train,y_train,sgd_size)\n",
    "            temp_x_grads=Lmodel.logistic_grads(x_sgd,y_sgd,model_params)      \n",
    "\n",
    "            if (i==0):\n",
    "                theta_cache=Lmodel.params_exp(model_params)**2\n",
    "                gradient_cache=Lmodel.params_exp(temp_x_grads)**2\n",
    "            else:\n",
    "                theta_cache=decay_rate*theta_cache+(1-decay_rate)*Lmodel.params_exp(model_params)\n",
    "                gradient_cache=decay_rate*gradient_cache+(1-decay_rate)*Lmodel.params_exp(temp_x_grads)\n",
    "\n",
    "            Lmodel.update_AdaDelta(temp_x_grads,theta_cache,model_params,gradient_cache)\n",
    "\n",
    "            train_loss_arr.append(train_loss)\n",
    "            val_loss_arr.append(val_loss)\n",
    "        outcome=(train_loss_arr,val_loss_arr)\n",
    "        return outcome\n",
    "    def Adam_train(x_train,y_train,x_val,y_val,model_params,iter_num,sgd_size,learning_rate,beta1,beta2):\n",
    "        '''\n",
    "        the whole train process\n",
    "        '''\n",
    "        train_loss_arr=[]\n",
    "        val_loss_arr=[]\n",
    "        m=[0.0]*len(x_train[0])\n",
    "        v=0\n",
    "        for i in range(iter_num):\n",
    "            train_loss=Lmodel.logistic_loss(x_train,y_train,model_params)\n",
    "            val_loss=Lmodel.logistic_loss(x_val,y_val,model_params)\n",
    "\n",
    "            x_sgd,y_sgd=Lmodel.get_sgd_iter_set(x_train,y_train,sgd_size)\n",
    "            temp_x_grads=Lmodel.logistic_grads(x_sgd,y_sgd,model_params)      \n",
    "\n",
    "            m,v=Lmodel.update_Adam(temp_x_grads,learning_rate,beta1,beta2,m,v)\n",
    "\n",
    "            train_loss_arr.append(train_loss)\n",
    "            val_loss_arr.append(val_loss)\n",
    "        outcome=(train_loss_arr,val_loss_arr)\n",
    "        return outcome\n",
    "\n",
    "    def draw_pic(train_list,val_list,name_list,iter_num):\n",
    "        '''\n",
    "        draw pic \n",
    "        '''\n",
    "        plt.title('TRAIN LOSS')  \n",
    "        #plt.title('VAL LOSS')  \n",
    "        plt.xlabel('iteration')  \n",
    "        plt.ylabel('avg loss')  \n",
    "        for i in range(len(name_list)):\n",
    "            plt.plot(range(iter_num), train_list[i],label=name_list[i]+'_train')  \n",
    "            #plt.plot(range(iter_num), val_list[i],label=name_list[i]+'_val')  \n",
    "\n",
    "        plt.xticks(range(iter_num), rotation=0)  \n",
    "        plt.legend(bbox_to_anchor=[0.3, 1])  \n",
    "        plt.grid()  \n",
    "        plt.show()  \n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":  \n",
    "    #load data\n",
    "    x_train,y_train=ds.load_svmlight_file(\"./a9a\")\n",
    "    x_train=x_train.toarray('c')\n",
    "    x_val,y_val=ds.load_svmlight_file(\"./a9a.t\")\n",
    "    x_val=x_val.toarray('c')\n",
    "    model_params=[0.0]*len(x_train[0])\n",
    "\n",
    "\n",
    "   \n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    name_loss_list=[]\n",
    "    iter_num=8\n",
    "    sgd_size=16\n",
    "    learning_rate=0.01\n",
    "    #norm train\n",
    "    norm_train_loss,norm_val_loss=Lmodel.norm_train(x_train,y_train,x_val,y_val,model_params,learning_rate,iter_num,sgd_size)\n",
    "    train_loss_list.append(norm_train_loss)\n",
    "    val_loss_list.append(norm_val_loss)\n",
    "    name_loss_list.append(\"norm method\")\n",
    "    \n",
    " \n",
    "    #for nag\n",
    "    model_params=[0.0]*len(x_train[0])\n",
    "    d_params=[0.0]*len(x_train[0])\n",
    "    belta=0.1\n",
    "    nag_train_loss,nag_val_loss=Lmodel.nag_train(x_train,y_train,x_val,y_val,model_params,d_params,learning_rate,iter_num,sgd_size,belta)\n",
    "    train_loss_list.append(nag_train_loss)\n",
    "    val_loss_list.append(nag_val_loss)\n",
    "    name_loss_list.append(\"nag method\")\n",
    "\n",
    "    #for RMSProp\n",
    "    model_params=[0.0]*len(x_train[0])\n",
    "    decay_rate=0.9\n",
    "    RMSProp_train_loss,RMSProp_val_loss=Lmodel.RMSProp_train(x_train,y_train,x_val,y_val,model_params,learning_rate,iter_num,sgd_size,decay_rate)\n",
    "    train_loss_list.append(RMSProp_train_loss)\n",
    "    val_loss_list.append(RMSProp_val_loss)\n",
    "    name_loss_list.append(\"RMSProp method\")\n",
    "\n",
    "    #for AdaDelta\n",
    "    model_params=[0.0]*len(x_train[0])\n",
    "    decay_rate=0.9\n",
    "    AdaDelta_train_loss,AdaDelta_val_loss=Lmodel.AdaDelta_train(x_train,y_train,x_val,y_val,model_params,learning_rate,iter_num,sgd_size,decay_rate)\n",
    "    train_loss_list.append(AdaDelta_train_loss)\n",
    "    val_loss_list.append(AdaDelta_val_loss)\n",
    "    name_loss_list.append(\"AdaDelta method\")\n",
    "\n",
    "    #for Adam\n",
    "    beta1=0.9\n",
    "    beta2=0.999\n",
    "    model_params=[0.0]*len(x_train[0])\n",
    "    decay_rate=0.9\n",
    "    Adam_train_loss,Adam_val_loss=Lmodel.Adam_train(x_train,y_train,x_val,y_val,model_params,iter_num,sgd_size,learning_rate,beta1,beta2)\n",
    "    train_loss_list.append(Adam_train_loss)\n",
    "    val_loss_list.append(Adam_val_loss)\n",
    "    name_loss_list.append(\"Adam method\")\n",
    "\n",
    "\n",
    "    Lmodel.draw_pic(train_loss_list,val_loss_list,name_loss_list,iter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
